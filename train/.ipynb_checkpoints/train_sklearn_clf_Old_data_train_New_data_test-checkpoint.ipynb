{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import csv\n",
    "from itertools import groupby\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "from sklearn.metrics import recall_score as rec, precision_score as pre, f1_score as f1, accuracy_score as acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Signatures and z scores\n",
    "csv_name = \"../data/labeled_sigs\"\n",
    "new_csv_name = \"../data/Adomain_Substrate_labeled_sigs.report\"\n",
    "z_score_csv_name = \"../data/z_score_aa\"\n",
    "df_original = pd.read_csv(csv_name, delimiter='\\t')\n",
    "df_new = pd.read_csv(new_csv_name, delimiter='\\t')\n",
    "z_scores_df = pd.read_csv(z_score_csv_name, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep for hardcoded values\n",
    "m=26\n",
    "nrow = 12\n",
    "D = np.zeros((nrow, m))\n",
    "D1 = D[0 ,:]\n",
    "D2 = D[1 ,:]\n",
    "D3 = D[2 ,:]\n",
    "D4 = D[3 ,:]\n",
    "D5 = D[4 ,:]\n",
    "D6 = D[5 ,:]\n",
    "D7 = D[6 ,:]\n",
    "D8 = D[7 ,:]\n",
    "D9 = D[8 ,:]\n",
    "D10 = D[9 ,:]\n",
    "D11 = D[10 ,:]\n",
    "D12 = D[11 ,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard coded values, taken from NRPSPredictor2 github\n",
    "\n",
    "#1 aa-alpha-helix.aaindex\n",
    "D1[0]=1.420;D1[1]=0.000;D1[2]=0.700;D1[3]=1.010;D1[4]=1.510;D1[5]=1.130;D1[6]=0.570;D1[7]=1.000;D1[8]=1.080;D1[9]=0.000;D1[10]=1.160;D1[11]=1.210;D1[12]=1.450;D1[13]=0.670;D1[14]=0.000;D1[15]=0.570;D1[16]=1.110;D1[17]=0.980;D1[18]=0.770;D1[19]=0.830;D1[20]=0.000;D1[21]=1.060;D1[22]=1.080;D1[23]=0.000;D1[24]=0.690;D1[25]=0.000;\n",
    "#2 aa-beta-sheet.aaindex\n",
    "D2[0]=0.830;D2[1]=0.000;D2[2]=1.190;D2[3]=0.540;D2[4]=0.370;D2[5]=1.380;D2[6]=0.750;D2[7]=0.870;D2[8]=1.600;D2[9]=0.000;D2[10]=0.740;D2[11]=1.300;D2[12]=1.050;D2[13]=0.890;D2[14]=0.000;D2[15]=0.550;D2[16]=1.100;D2[17]=0.930;D2[18]=0.750;D2[19]=1.190;D2[20]=0.000;D2[21]=1.700;D2[22]=1.370;D2[23]=0.000;D2[24]=1.470;D2[25]=0.000;\n",
    "#3 aa-beta-turn.aaindex\n",
    "D3[0]=0.740;D3[1]=0.000;D3[2]=0.960;D3[3]=1.520;D3[4]=0.950;D3[5]=0.660;D3[6]=1.560;D3[7]=0.950;D3[8]=0.470;D3[9]=0.000;D3[10]=1.190;D3[11]=0.500;D3[12]=0.600;D3[13]=1.460;D3[14]=0.000;D3[15]=1.560;D3[16]=0.960;D3[17]=1.010;D3[18]=1.430;D3[19]=0.980;D3[20]=0.000;D3[21]=0.590;D3[22]=0.600;D3[23]=0.000;D3[24]=1.140;D3[25]=0.000;\n",
    "#4 aa-hydrogenbond.aaindex\n",
    "D4[0]=0.000;D4[1]=0.000;D4[2]=0.000;D4[3]=1.000;D4[4]=1.000;D4[5]=0.000;D4[6]=0.000;D4[7]=1.000;D4[8]=0.000;D4[9]=0.000;D4[10]=2.000;D4[11]=0.000;D4[12]=0.000;D4[13]=2.000;D4[14]=0.000;D4[15]=0.000;D4[16]=2.000;D4[17]=4.000;D4[18]=1.000;D4[19]=1.000;D4[20]=0.000;D4[21]=0.000;D4[22]=1.000;D4[23]=0.000;D4[24]=1.000;D4[25]=0.000;\n",
    "#5 aa-hydrophobicity-neu1.aaindex\n",
    "D5[0]=0.060;D5[1]=0.000;D5[2]=-0.560;D5[3]=0.970;D5[4]=0.850;D5[5]=-0.990;D5[6]=0.320;D5[7]=0.150;D5[8]=-1.000;D5[9]=0.000;D5[10]=1.000;D5[11]=-0.830;D5[12]=-0.680;D5[13]=0.700;D5[14]=0.000;D5[15]=0.450;D5[16]=0.710;D5[17]=0.800;D5[18]=0.480;D5[19]=0.380;D5[20]=0.000;D5[21]=-0.750;D5[22]=-0.570;D5[23]=0.000;D5[24]=-0.350;D5[25]=0.000;\n",
    "#6 aa-hydrophobicity-neu2.aaindex\n",
    "D6[0]=-0.250;D6[1]=0.000;D6[2]=-0.400;D6[3]=-0.080;D6[4]=-0.100;D6[5]=0.180;D6[6]=-0.320;D6[7]=-0.030;D6[8]=-0.030;D6[9]=0.000;D6[10]=0.320;D6[11]=0.050;D6[12]=-0.010;D6[13]=-0.060;D6[14]=0.000;D6[15]=0.230;D6[16]=-0.020;D6[17]=0.190;D6[18]=-0.150;D6[19]=-0.100;D6[20]=0.000;D6[21]=-0.190;D6[22]=0.310;D6[23]=0.000;D6[24]=0.400;D6[25]=0.000;\n",
    "#7 aa-hydrophobicity-neu3.aaindex\n",
    "D7[0]=0.250;D7[1]=0.000;D7[2]=-0.140;D7[3]=0.080;D7[4]=-0.050;D7[5]=0.150;D7[6]=0.280;D7[7]=-0.100;D7[8]=0.100;D7[9]=0.000;D7[10]=0.110;D7[11]=0.010;D7[12]=0.040;D7[13]=0.170;D7[14]=0.000;D7[15]=0.410;D7[16]=0.120;D7[17]=-0.410;D7[18]=0.230;D7[19]=0.290;D7[20]=0.000;D7[21]=0.030;D7[22]=0.340;D7[23]=0.000;D7[24]=-0.020;D7[25]=0.000;\n",
    "#8 aa-isoelectric.aaindex\n",
    "D8[0]=6.000;D8[1]=0.000;D8[2]=5.050;D8[3]=2.770;D8[4]=3.220;D8[5]=5.480;D8[6]=5.970;D8[7]=7.590;D8[8]=6.020;D8[9]=0.000;D8[10]=9.740;D8[11]=5.980;D8[12]=5.740;D8[13]=5.410;D8[14]=0.000;D8[15]=6.300;D8[16]=5.650;D8[17]=10.760;D8[18]=5.680;D8[19]=5.660;D8[20]=0.000;D8[21]=5.960;D8[22]=5.890;D8[23]=0.000;D8[24]=5.660;D8[25]=0.000;\n",
    "#9 aa-polar-grantham.aaindex\n",
    "D9[0]=8.100;D9[1]=0.000;D9[2]=5.500;D9[3]=13.000;D9[4]=12.300;D9[5]=5.200;D9[6]=9.000;D9[7]=10.400;D9[8]=5.200;D9[9]=0.000;D9[10]=11.300;D9[11]=4.900;D9[12]=5.700;D9[13]=11.600;D9[14]=0.000;D9[15]=8.000;D9[16]=10.500;D9[17]=10.500;D9[18]=9.200;D9[19]=8.600;D9[20]=0.000;D9[21]=5.900;D9[22]=5.400;D9[23]=0.000;D9[24]=6.200;D9[25]=0.000;\n",
    "#10 aa-polar-radzicka.aaindex\n",
    "D10[0]=-0.060;D10[1]=0.000;D10[2]=1.360;D10[3]=-0.800;D10[4]=-0.770;D10[5]=1.270;D10[6]=-0.410;D10[7]=0.490;D10[8]=1.310;D10[9]=0.000;D10[10]=-1.180;D10[11]=1.210;D10[12]=1.270;D10[13]=-0.480;D10[14]=0.000;D10[15]=1.100;D10[16]=-0.730;D10[17]=-0.840;D10[18]=-0.500;D10[19]=-0.270;D10[20]=0.000;D10[21]=1.090;D10[22]=0.880;D10[23]=0.000;D10[24]=0.330;D10[25]=0.000;\n",
    "#11 aa-polar-zimmerman.aaindex\n",
    "D11[0]=0.000;D11[1]=0.000;D11[2]=1.480;D11[3]=49.700;D11[4]=49.900;D11[5]=0.350;D11[6]=0.000;D11[7]=51.600;D11[8]=0.130;D11[9]=0.000;D11[10]=49.500;D11[11]=0.130;D11[12]=1.430;D11[13]=3.380;D11[14]=0.000;D11[15]=1.580;D11[16]=3.530;D11[17]=52.000;D11[18]=1.670;D11[19]=1.660;D11[20]=0.000;D11[21]=0.130;D11[22]=2.100;D11[23]=0.000;D11[24]=1.610;D11[25]=0.000;\n",
    "#12 aa-volume.aaindex\n",
    "D12[0]=90.000;D12[1]=0.000;D12[2]=103.300;D12[3]=117.300;D12[4]=142.200;D12[5]=191.900;D12[6]=64.900;D12[7]=160.000;D12[8]=163.900;D12[9]=0.000;D12[10]=167.300;D12[11]=164.000;D12[12]=167.000;D12[13]=124.700;D12[14]=0.000;D12[15]=122.900;D12[16]=149.400;D12[17]=194.000;D12[18]=95.400;D12[19]=121.500;D12[20]=0.000;D12[21]=139.000;D12[22]=228.200;D12[23]=0.000;D12[24]=197.000;D12[25]=0.000;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Amino acid abbreviations with indices of D[i, :]\n",
    "\n",
    "aa2ind_map = {}\n",
    "aa2ind_map['A'] = 0\n",
    "aa2ind_map['R'] = 17\n",
    "aa2ind_map['D'] = 3\n",
    "aa2ind_map['N'] = 13\n",
    "aa2ind_map['C'] = 2\n",
    "aa2ind_map['E'] = 4\n",
    "aa2ind_map['Q'] = 16\n",
    "aa2ind_map['G'] = 6\n",
    "aa2ind_map['H'] = 7\n",
    "aa2ind_map['I'] = 8\n",
    "aa2ind_map['L'] = 11\n",
    "aa2ind_map['K'] = 10\n",
    "aa2ind_map['M'] = 12\n",
    "aa2ind_map['F'] = 5\n",
    "aa2ind_map['P'] = 15\n",
    "aa2ind_map['S'] = 18\n",
    "aa2ind_map['T'] = 19\n",
    "aa2ind_map['W'] = 22\n",
    "aa2ind_map['Y'] = 24\n",
    "aa2ind_map['V'] = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store 15 dimensional vector values for each amino acid\n",
    "\n",
    "aa_dict = {}\n",
    "for i, row in z_scores_df.iterrows():\n",
    "    aa_dict[row['aa_short']] = [row['z1'], row['z2'], row['z3'], ] # Store z values\n",
    "    aa_dict[row['aa_short']].extend(D[:, aa2ind_map[row['aa_short']]]) # Store rest of the 12 hard-coded values\n",
    "aa_dict['-'] = list(np.mean(np.asarray(list(aa_dict.values())), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index to value map and value to index map from a list using list index\n",
    "def get_dict_from_list(mylist):\n",
    "    assert len(mylist) > 0\n",
    "    idx_to_value_dict = {}\n",
    "    value_to_idx_dict = {}\n",
    "    for i in range(len(mylist)):\n",
    "        idx_to_value_dict[i] = mylist[i]\n",
    "        value_to_idx_dict[mylist[i]] = int(i)\n",
    "    return idx_to_value_dict, value_to_idx_dict\n",
    "\n",
    "# Get index-value maps for substrates\n",
    "sub_vocab_set = set(df_original['sub'].tolist() + ['null'])\n",
    "sub_vocab_list = list(sub_vocab_set)\n",
    "sub_idx_to_value_dict, sub_value_to_idx_dict = get_dict_from_list(sub_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cys': 0,\n",
       " 'lysKb': 1,\n",
       " 'ser': 2,\n",
       " 'dab': 3,\n",
       " 'bht': 4,\n",
       " 'dpg': 5,\n",
       " 'pip': 6,\n",
       " 'tcl': 7,\n",
       " 'arg': 8,\n",
       " 'ala': 9,\n",
       " '3KmeKglu': 10,\n",
       " 'trp/tyr': 11,\n",
       " 'dhpg': 12,\n",
       " 'alaKb': 13,\n",
       " 'abu': 14,\n",
       " 'trp': 15,\n",
       " 'dKlyserg': 16,\n",
       " 'vol': 17,\n",
       " 'val/ile': 18,\n",
       " 'asp': 19,\n",
       " 'dhb': 20,\n",
       " 'alloKthr': 21,\n",
       " '2KoxoKisovalericKacid': 22,\n",
       " 'hivKd': 23,\n",
       " 'lys': 24,\n",
       " 'hpg': 25,\n",
       " 'null': 26,\n",
       " 'ile': 27,\n",
       " 'phg': 28,\n",
       " 'aeo': 29,\n",
       " 'serKthr': 30,\n",
       " 'haorn': 31,\n",
       " 'hforn': 32,\n",
       " 'aad': 33,\n",
       " 'dht': 34,\n",
       " 'betaKala': 35,\n",
       " 'phe': 36,\n",
       " 'asn/gln': 37,\n",
       " 'pro': 38,\n",
       " 'LDAP': 39,\n",
       " 'iva': 40,\n",
       " 'leu': 41,\n",
       " 'val/ile/alloile': 42,\n",
       " 'horn': 43,\n",
       " 'hasn': 44,\n",
       " 'thr': 45,\n",
       " 'tyr': 46,\n",
       " 'glu': 47,\n",
       " 'his': 48,\n",
       " 'alaninol': 49,\n",
       " 'sal': 50,\n",
       " 'gly': 51,\n",
       " '4ppro': 52,\n",
       " 'gln': 53,\n",
       " 'NK(1,1KdimethylK1Kallyl)Trp': 54,\n",
       " 'cap': 55,\n",
       " 'alphaKhydroxyKisocaproic acid': 56,\n",
       " 'val': 57,\n",
       " 'orn': 58,\n",
       " 'asn': 59}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_value_to_idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a signature of amino acids(34 aa long), construct the 34x15=510 dimentional representation\n",
    "def get_encoding(signature):\n",
    "    ret = []\n",
    "    for i in signature:\n",
    "        ret.extend(aa_dict[i])\n",
    "    return np.asarray(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_sub(sub_in):\n",
    "    if sub_in.lower() == 'beta-ala':\n",
    "        ret = 'ala'\n",
    "    elif sub_in.lower() == 'orn':\n",
    "        ret = 'orn'\n",
    "    elif sub_in.lower() == 'hyv-d':\n",
    "        ret = 'null'\n",
    "    else:\n",
    "        ret = sub_in\n",
    "    \n",
    "    return ret\n",
    "\n",
    "# Format the data\n",
    "def format_data_from_df(df_in):\n",
    "    sig_len = 34\n",
    "    raw_data = []\n",
    "    data_np = []\n",
    "    label_np = []\n",
    "    j=0\n",
    "    for i, row in df_in.iterrows():\n",
    "        assert len(row['sig']) == sig_len\n",
    "        raw_data.append(row['sig'])\n",
    "        data_np.append(get_encoding(row['sig']))\n",
    "        label_np.append(sub_value_to_idx_dict[get_correct_sub(row['sub'])])\n",
    "        j+=1\n",
    "    return np.asarray(data_np), np.asarray(label_np).astype('int'), np.array(raw_data)\n",
    "\n",
    "data_np_nrps, label_np_nrps, raw_data_nrps = format_data_from_df(df_original)\n",
    "data_np_test, label_np_test, raw_data_test = format_data_from_df(df_new)\n",
    "#data_np = np.asarray(data_np)\n",
    "#label_np = np.asarray(label_np).astype('int')\n",
    "#raw_data = np.array(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hamming_distance(str1, str2):\n",
    "    return sum(i != j for i, j in zip(str1, str2))\n",
    "\n",
    "# For all points in test_data, choose the minimum hamming distance from all of train data, and return the distance list\n",
    "def get_hamming_distance_bucket_info(test_data, train_data):\n",
    "    dist_list = []\n",
    "    for test_data_pt in test_data:\n",
    "        dist_list.append(min([get_hamming_distance(test_data_pt, train_data_pt) for train_data_pt in train_data]))\n",
    "    return np.array(dist_list)\n",
    "\n",
    "def round_dec(num, dec=2):\n",
    "    return float(round(num* 10.**dec))/(10**dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build classifier dictionary using sklearn multiclass classifiers\n",
    "\n",
    "clf_dict={}\n",
    "clf_dict['lr'] = LogisticRegression(random_state=0, max_iter=400, multi_class='multinomial', solver='newton-cg')\n",
    "clf_dict['svm'] = make_pipeline(StandardScaler(), LinearSVC(random_state=0, multi_class='crammer_singer', tol=1e-9, max_iter=2000))\n",
    "clf_dict['knn'] = KNeighborsClassifier(weights='distance')\n",
    "clf_dict['mlp_sklearn'] = MLPClassifier(random_state=1, max_iter=400, early_stopping=False, )\n",
    "clf_dict['rand_for'] = RandomForestClassifier(max_depth=4, criterion='entropy')\n",
    "clf_dict['dec_tree'] = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "clf_dict['ber_nb'] = BernoulliNB()\n",
    "clf_dict['xtra_tree'] = ExtraTreesClassifier(n_estimators=100, random_state=0, criterion='gini')\n",
    "clf_dict['gau_nb'] = GaussianNB()\n",
    "clf_dict['label_prop'] = LabelPropagation(kernel='knn')\n",
    "clf_dict['label_spread'] = LabelSpreading(kernel='knn')\n",
    "clf_dict['lda'] = LinearDiscriminantAnalysis()\n",
    "clf_dict['ridge_cv'] = RidgeClassifierCV()\n",
    "clf_dict['n_cent'] = NearestCentroid()\n",
    "clf_dict['ridge'] = RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_stats(metric_list, metric_list_bucket, n_bucket):\n",
    "    print(f\"\\n\\nOverall Stats:\")\n",
    "\n",
    "    for item in metric_list:\n",
    "        print(f\"Test percentage: {item['pct']:.2f} Average Accuracy: {item['acc']:.3f} Average Recall: {item['rec']:.3f} Average Precision: {item['pre']:.3f} Average F1 Score: {item['f1']:.3f}\")\n",
    "\n",
    "    for bucket in range(n_bucket):\n",
    "        print(f\"\\n\\nBucket {bucket+1} Stats:\")\n",
    "        for item in metric_list_bucket:\n",
    "            print(f\"Test percentage: {item['pct']:.2f} Average Accuracy: {item['acc'][bucket]:.3f} Average Recall: {item['rec'][bucket]:.3f} Average Precision: {item['pre'][bucket]:.3f} Average F1 Score: {item['f1'][bucket]:.3f}\")\n",
    "\n",
    "def save_stats(metric_list, metric_list_bucket, n_bucket, pct_list, csv_name='stats'):\n",
    "    overall_dict = {}\n",
    "    for item in metric_list:\n",
    "        overall_dict[item['pct']] = item['acc']\n",
    "    \n",
    "    with open(csv_name + '.csv', 'w', newline='') as csvfile:\n",
    "        fieldnames = ['pct', 'overall_acc'] + ['Bucket '+str(item) for item in range(n_bucket)]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for item in metric_list_bucket:\n",
    "            temp_dict = {'pct': item['pct'], 'overall_acc': round_dec(overall_dict[item['pct']])}\n",
    "            for bucket in range(n_bucket):\n",
    "                temp_dict['Bucket ' + str(bucket)] = round_dec(item['acc'][bucket])\n",
    "            writer.writerow(temp_dict)\n",
    "\n",
    "# n_iter signifies number of iterations\n",
    "# test_percentage_list is the list of percentages of test data with respect to total data\n",
    "# Code will iterate for each percentage for n_iter iterations\n",
    "def train_and_validate(clf_type, n_iter=10, test_percentage_list = [5, 10, 25, 40, 50], print_stat=True, save_stat=True):\n",
    "    print(f'\\nUsing {clf_type} classifier')\n",
    "    #n=len(label_np)\n",
    "    avg = 'micro'\n",
    "    # metric_list contains overall accuracy, precision, recall and f1 score for all data\n",
    "    # metric_list_bucket contains hammning distance bucket-wise accuracy, precision, recall and f1 score\n",
    "    metric_list = []\n",
    "    metric_list_bucket = []\n",
    "    n_bucket = 16\n",
    "    eps = 1e-8\n",
    "    test_percentage_list = [round_dec(len(data_np_test)/(len(data_np_test)+len(data_np_nrps)))]\n",
    "    #n_iter = 5\n",
    "\n",
    "    for test_percentage in test_percentage_list:\n",
    "        assert test_percentage>0 and test_percentage<100\n",
    "        acc_sum = 0\n",
    "        pre_sum = 0\n",
    "        rec_sum = 0\n",
    "        f1_sum = 0\n",
    "        print(f'Test data percentage wrt total data: {test_percentage}')\n",
    "        n_iter_ar = np.zeros(n_bucket)\n",
    "        acc_sum_ar = np.zeros(n_bucket)\n",
    "        pre_sum_ar = np.zeros(n_bucket)\n",
    "        rec_sum_ar = np.zeros(n_bucket)\n",
    "        f1_sum_ar = np.zeros(n_bucket)\n",
    "        dist_buckets_all = []\n",
    "\n",
    "        for iter in range(n_iter):\n",
    "            # Create filter for random split\n",
    "            '''\n",
    "            test_elig = np.random.random(size=(n)) <= (test_percentage/100)\n",
    "            test_data = data_np[test_elig]\n",
    "            test_label = label_np[test_elig]\n",
    "            train_data = data_np[(test_elig-1).astype('bool')]\n",
    "            train_label = label_np[(test_elig-1).astype('bool')]\n",
    "\n",
    "            raw_train_data = raw_data[(test_elig-1).astype('bool')]\n",
    "            raw_test_data = raw_data[test_elig]\n",
    "            '''\n",
    "            test_data, test_label, raw_test_data = data_np_test, label_np_test, raw_data_test\n",
    "            train_data, train_label, raw_train_data = data_np_nrps, label_np_nrps, raw_data_nrps\n",
    "            \n",
    "            dist_buckets = get_hamming_distance_bucket_info(raw_test_data, raw_train_data)\n",
    "            dist_buckets_all.extend(list(dist_buckets))\n",
    "            #print(dist_buckets)\n",
    "\n",
    "            clf = clf_dict[clf_type]\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                clf.fit(train_data, train_label)\n",
    "            test_predicted = clf.predict(test_data)\n",
    "\n",
    "            accuracy = acc(test_label, test_predicted)\n",
    "            precision = pre(test_label, test_predicted, average=avg)\n",
    "            recall = rec(test_label, test_predicted, average=avg)\n",
    "            f1_score = f1(test_label, test_predicted, average=avg)\n",
    "            acc_sum += accuracy\n",
    "            pre_sum += precision\n",
    "            rec_sum += recall\n",
    "            f1_sum += f1_score\n",
    "            print(f'Accuracy: {accuracy:.3f} Recall: {recall:.3f} Precision: {precision:.3f} F1 Score: {f1_score:.3f}')\n",
    "\n",
    "            for bucket in range(n_bucket):\n",
    "                b_filter = dist_buckets == bucket\n",
    "                filtered_test_label = test_label[b_filter]\n",
    "                filtered_test_predicted = test_predicted[b_filter]\n",
    "                if len(filtered_test_label):\n",
    "                    accuracy = acc(filtered_test_label, filtered_test_predicted)\n",
    "                    precision = pre(filtered_test_label, filtered_test_predicted, average=avg)\n",
    "                    recall = rec(filtered_test_label, filtered_test_predicted, average=avg)\n",
    "                    f1_score = f1(filtered_test_label, filtered_test_predicted, average=avg)\n",
    "                    acc_sum_ar[bucket] += accuracy\n",
    "                    pre_sum_ar[bucket] += precision\n",
    "                    rec_sum_ar[bucket] += recall\n",
    "                    f1_sum_ar[bucket] += f1_score\n",
    "                    n_iter_ar[bucket] += 1\n",
    "\n",
    "\n",
    "        metric_list.append({'pct':test_percentage, 'acc':acc_sum/n_iter, 'pre':pre_sum/n_iter, 'rec':rec_sum/n_iter, 'f1':f1_sum/n_iter})\n",
    "        metric_list_bucket.append({'pct':test_percentage, 'acc':acc_sum_ar/(n_iter+eps), 'pre':pre_sum_ar/(n_iter+eps), 'rec':rec_sum_ar/(n_iter+eps), 'f1':f1_sum_ar/(n_iter+eps)})\n",
    "        freq_stat = {value: round_dec(len(list(freq))*100./len(dist_buckets_all), 2) for value, freq in groupby(sorted(dist_buckets_all))}\n",
    "        print(f\"Frequency of hamming distance: \", freq_stat)\n",
    "\n",
    "\n",
    "    if print_stat:\n",
    "        print_stats(metric_list, metric_list_bucket, n_bucket)\n",
    "    if save_stat:\n",
    "        save_stats(metric_list, metric_list_bucket, n_bucket, test_percentage_list, csv_name='../results/T_Adomain_old_new_stats_'+clf_type+'_iter_'+str(n_iter)+'_pct_'+'_'.join(map(str, test_percentage_list)))\n",
    "    return {'overall':metric_list, 'bucket':metric_list_bucket}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_consolidated_stats(metric_struct, metric_bucket_struct):\n",
    "    algos = list(metric_struct.keys())\n",
    "    if algos == []:\n",
    "        return\n",
    "    pct_list = [item['pct'] for item in metric_struct[algos[0]]]\n",
    "    n_bucket = len(metric_bucket_struct[algos[0]][0]['acc'])\n",
    "    overall_struct = {}\n",
    "    bucket_struct = {}\n",
    "    for algo in algos:\n",
    "        temp = {}\n",
    "        for item in metric_struct[algo]:\n",
    "            temp[item['pct']] = round_dec(item['acc'])\n",
    "        overall_struct[algo] = temp\n",
    "        temp = {}\n",
    "        for item in metric_bucket_struct[algo]:\n",
    "            temp[item['pct']] = [round_dec(it) for it in item['acc']]\n",
    "        bucket_struct[algo] = temp\n",
    "    for pct in pct_list:\n",
    "        with open('../results/T_Adomain_old_new_consolidated_stats_algo_'+'_'.join(map(str, algos))+'pct_'+str(pct)+'_bucket_'+str(n_bucket)+ '.csv', 'w', newline='') as csvfile:\n",
    "            Bucket_fields = ['Bucket '+str(item) for item in range(n_bucket)]\n",
    "            fieldnames = ['Algo', 'Overall'] + Bucket_fields\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "            for algo in algos:\n",
    "                temp = {'Algo': algo, 'Overall':overall_struct[algo][pct]}\n",
    "                temp.update(dict(zip(Bucket_fields, bucket_struct[algo][pct])))\n",
    "                writer.writerow(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using lr classifier\n",
      "Test data percentage wrt total data: 0.77\n",
      "Accuracy: 0.812 Recall: 0.812 Precision: 0.812 F1 Score: 0.812\n",
      "Accuracy: 0.812 Recall: 0.812 Precision: 0.812 F1 Score: 0.812\n"
     ]
    }
   ],
   "source": [
    "algo_overall_metric = {}\n",
    "algo_bucket_metric = {}\n",
    "for clf in list(clf_dict.keys()):\n",
    "#for clf in ['lr']:\n",
    "    ret = train_and_validate(clf, n_iter=1)\n",
    "    algo_overall_metric[clf] = ret['overall']\n",
    "    algo_bucket_metric[clf] = ret['bucket']\n",
    "save_consolidated_stats(algo_overall_metric, algo_bucket_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
